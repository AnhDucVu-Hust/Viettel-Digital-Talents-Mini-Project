{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKfAsPApOd29",
        "outputId": "33d9a075-c02f-4d04-e673-33bb5c17cb74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vncorenlp\n",
            "  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2022.6.15)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645951 sha256=f85149659eda5d9e8c47dae8450f05b4413ac44e92a8e44eabbc177334a6008a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/d8/f2/d28d97379b4f6479bf51247c8dfd57fa00932fa7a74b6aab29\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n",
            "--2022-06-21 07:09:23--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M   116MB/s    in 0.2s    \n",
            "\n",
            "2022-06-21 07:09:23 (116 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2022-06-21 07:09:23--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-06-21 07:09:23 (12.1 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2022-06-21 07:09:24--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-06-21 07:09:24 (5.61 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install vncorenlp\n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/\n",
        "\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbsqTCEAOif3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from vncorenlp import VnCoreNLP\n",
        "rdrsegmenter = VnCoreNLP(\"/content/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "import numpy as np\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY3njkxSOsvu"
      },
      "outputs": [],
      "source": [
        "data=pd.read_excel(\"/Data/data.xlsx\")\n",
        "X=list(data['text'])\n",
        "y=list(data['Cấp 2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q2yhp74OwRD"
      },
      "outputs": [],
      "source": [
        "def standard_data(data):\n",
        "    for id in range(len(data)):\n",
        "        data[id] = re.sub(r\"[\\.,\\?]+$-\", \"\", data[id])\n",
        "        data[id] = data[id].replace(\",\", \" \").replace(\".\", \" \") \\\n",
        "            .replace(\";\", \" \").replace(\"“\", \" \") \\\n",
        "            .replace(\":\", \" \").replace(\"”\", \" \") \\\n",
        "            .replace('\"', \" \").replace(\"'\", \" \") \\\n",
        "            .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
        "            .replace(\"-\", \" \").replace(\"|\", \" \")\n",
        "        data[id] = data[id].strip().lower()\n",
        "        data[id] = re.sub(r'\\s\\s+', ' ', data[id])\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szfBEXXjPN00"
      },
      "outputs": [],
      "source": [
        "def remove_stopword(texts,stop_word):\n",
        "  data=[]\n",
        "  for i in tqdm(range(len(texts))):\n",
        "    data.append(\" \".join([word for word in texts[i].split() if word not in stop_word]))\n",
        "  return data\n",
        "with open(\"/Data/stopword.txt\",encoding=\"UTF-8\") as f:\n",
        "    stop_word=f.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwBvHB33PUC9"
      },
      "outputs": [],
      "source": [
        "def syllable(texts):\n",
        "  text=[]\n",
        "  for line in texts:\n",
        "    segmen=rdrsegmenter.tokenize(line)\n",
        "    segmen = ' '.join([' '.join(x) for x in segmen])\n",
        "    text.append(segmen)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZIjHf4XPvhR",
        "outputId": "f0b46f13-2148-409b-ae7e-56f450f34bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7217/7217 [01:23<00:00, 86.83it/s]\n"
          ]
        }
      ],
      "source": [
        "y=list(y)\n",
        "X=standard_data(X)\n",
        "X=remove_stopword(X,stop_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsFV9fCbPzcO"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR6hWGyjRK-G"
      },
      "outputs": [],
      "source": [
        "with open(\"/Data/viettel_train_input_no_tokenize.txt\",\"w\",encoding='UTF-8') as f:\n",
        "  for line in X_train:\n",
        "    f.write(line+\"\\n\")\n",
        "with open(\"/Data/viettel_train_label.txt\",\"w\",encoding='UTF-8') as f:\n",
        "    for line in y_train:\n",
        "      f.write(line+\"\\n\")\n",
        "with open(\"/Data/viettel_test_input_no_tokenize.txt\",\"w\",encoding='UTF-8') as f:\n",
        "    for line in X_test:\n",
        "      f.write(line+\"\\n\")\n",
        "with open(\"/Data/viettel_test_label.txt\",\"w\",encoding='UTF-8') as f:\n",
        "    for line in y_test:\n",
        "      f.write(line+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RGJMh7AQCOw"
      },
      "outputs": [],
      "source": [
        "X_train=syllable(X_train)\n",
        "X_test=syllable(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6m7cskgRTCD"
      },
      "outputs": [],
      "source": [
        "with open(\"/Data/viettel_train_input.txt\",\"w\",encoding='UTF-8') as f:\n",
        "  for line in X_train:\n",
        "    f.write(line+\"\\n\")\n",
        "with open(\"/Data/viettel_test_input.txt\",\"w\",encoding='UTF-8') as f:\n",
        "  for line in X_test:\n",
        "    f.write(line+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "W_cx5DtrhkNK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Preprocessing",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}