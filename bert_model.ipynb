{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install fastBPE\n!pip install fairseq\n\n!pip install vncorenlp\n!mkdir -p vncorenlp/models/wordsegmenter\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n!mv vi-vocab vncorenlp/models/wordsegmenter/\n!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-20T12:59:34.078552Z","iopub.execute_input":"2022-06-20T12:59:34.079247Z","iopub.status.idle":"2022-06-20T13:00:46.692007Z","shell.execute_reply.started":"2022-06-20T12:59:34.079156Z","shell.execute_reply":"2022-06-20T13:00:46.690939Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from vncorenlp import VnCoreNLP\nrdrsegmenter = VnCoreNLP(\"/kaggle/working/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:18:03.106640Z","iopub.execute_input":"2022-06-20T18:18:03.107609Z","iopub.status.idle":"2022-06-20T18:18:08.671572Z","shell.execute_reply.started":"2022-06-20T18:18:03.107563Z","shell.execute_reply":"2022-06-20T18:18:08.670489Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"!wget https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n!tar -xzvf PhoBERT_base_transformers.tar.gz","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:05:11.207197Z","iopub.execute_input":"2022-06-20T13:05:11.207585Z","iopub.status.idle":"2022-06-20T13:05:36.344351Z","shell.execute_reply.started":"2022-06-20T13:05:11.207527Z","shell.execute_reply":"2022-06-20T13:05:36.343398Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from fairseq.data.encoders.fastbpe import fastBPE\nfrom fairseq.data import Dictionary\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--bpe-codes', \n    default=\"/kaggle/working/PhoBERT_base_transformers/bpe.codes\",\n    required=False,\n    type=str,\n    help='path to fastBPE BPE'\n)\nargs, unknown = parser.parse_known_args()\nbpe = fastBPE(args)\n\n# Load the dictionary\nvocab = Dictionary()\nvocab.add_from_file(\"/kaggle/working/PhoBERT_base_transformers/dict.txt\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:06:30.498415Z","iopub.execute_input":"2022-06-20T13:06:30.499213Z","iopub.status.idle":"2022-06-20T13:06:30.706979Z","shell.execute_reply.started":"2022-06-20T13:06:30.499180Z","shell.execute_reply":"2022-06-20T13:06:30.706088Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndata=pd.read_excel(\"../input/vietteldata/data.xlsx\")\nX=list(data['text'])\ny=list(data['Cấp 2'])","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:07:48.866495Z","iopub.execute_input":"2022-06-20T13:07:48.866922Z","iopub.status.idle":"2022-06-20T13:07:50.380716Z","shell.execute_reply.started":"2022-06-20T13:07:48.866887Z","shell.execute_reply":"2022-06-20T13:07:50.379894Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import re\ndef standard_data(data):\n    for id in range(len(data)):\n        data[id] = re.sub(r\"[\\.,\\?]+$-\", \"\", data[id])\n        data[id] = data[id].replace(\",\", \" \").replace(\".\", \" \") \\\n            .replace(\";\", \" \").replace(\"“\", \" \") \\\n            .replace(\":\", \" \").replace(\"”\", \" \") \\\n            .replace('\"', \" \").replace(\"'\", \" \") \\\n            .replace(\"!\", \" \").replace(\"?\", \" \") \\\n            .replace(\"-\", \" \").replace(\"?\", \" \") \\\n            .replace(\"|\",\" \")\n        data[id] = data[id].strip().lower()\n        data[id] = re.sub(r'\\s\\s+', ' ', data[id])\n    return data\nX=standard_data(X)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:07:57.836040Z","iopub.execute_input":"2022-06-20T13:07:57.836682Z","iopub.status.idle":"2022-06-20T13:07:58.999902Z","shell.execute_reply.started":"2022-06-20T13:07:57.836644Z","shell.execute_reply":"2022-06-20T13:07:58.999078Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\nX_new=[]\nfor line in X:\n  segmen=rdrsegmenter.tokenize(line)\n  segmen = ' '.join([' '.join(x) for x in segmen])\n  X_new.append(segmen)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:08:09.845901Z","iopub.execute_input":"2022-06-20T13:08:09.846267Z","iopub.status.idle":"2022-06-20T13:09:31.985747Z","shell.execute_reply.started":"2022-06-20T13:08:09.846239Z","shell.execute_reply":"2022-06-20T13:09:31.984921Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X_new,y,test_size=0.2,random_state=44)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:14:01.708604Z","iopub.execute_input":"2022-06-20T13:14:01.709196Z","iopub.status.idle":"2022-06-20T13:14:01.722051Z","shell.execute_reply.started":"2022-06-20T13:14:01.709159Z","shell.execute_reply":"2022-06-20T13:14:01.721164Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nMAX_LEN = 250\ntrain_ids = []\nfor sent in X_train:\n    subwords = '<s> ' + bpe.encode(sent) + ' </s>'\n    encoded_sent = vocab.encode_line(subwords, append_eos=True, add_if_not_exist=False).long().tolist()\n    train_ids.append(encoded_sent)\n\ntest_ids = []\nfor sent in X_test:\n    subwords = '<s> ' + bpe.encode(sent) + ' </s>'\n    encoded_sent = vocab.encode_line(subwords, append_eos=True, add_if_not_exist=False).long().tolist()\n    test_ids.append(encoded_sent)\n    \ntrain_ids = pad_sequences(train_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\ntest_ids = pad_sequences(test_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:14:01.773469Z","iopub.execute_input":"2022-06-20T13:14:01.773913Z","iopub.status.idle":"2022-06-20T13:14:23.207002Z","shell.execute_reply.started":"2022-06-20T13:14:01.773885Z","shell.execute_reply":"2022-06-20T13:14:23.206060Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"train_masks = []\nfor sent in train_ids:\n    mask = [int(token_id > 0) for token_id in sent]\n    train_masks.append(mask)\n\ntest_masks = []\nfor sent in test_ids:\n    mask = [int(token_id > 0) for token_id in sent]\n\n    test_masks.append(mask)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:14:23.209013Z","iopub.execute_input":"2022-06-20T13:14:23.209548Z","iopub.status.idle":"2022-06-20T13:14:24.414234Z","shell.execute_reply.started":"2022-06-20T13:14:23.209485Z","shell.execute_reply":"2022-06-20T13:14:24.413409Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"labels=list(set(y))\nlabel=list(set(y_train))\nlabel2id=dict([label,id] for id,label in enumerate(label))\ny_train=[label2id[label] for label in y_train]\ny_test=[label2id[label] for label in y_test]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:14:24.415663Z","iopub.execute_input":"2022-06-20T13:14:24.416026Z","iopub.status.idle":"2022-06-20T13:14:24.422999Z","shell.execute_reply.started":"2022-06-20T13:14:24.415988Z","shell.execute_reply":"2022-06-20T13:14:24.422187Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch\ntrain_inputs = torch.tensor(train_ids)\ntest_inputs = torch.tensor(test_ids)\ntrain_labels = torch.tensor(y_train)\ntest_labels = torch.tensor(y_test)\ntrain_masks = torch.tensor(train_masks)\ntest_masks = torch.tensor(test_masks)\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = SequentialSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n\ntest_data = TensorDataset(test_inputs, test_masks, test_labels)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:14:24.425225Z","iopub.execute_input":"2022-06-20T13:14:24.425597Z","iopub.status.idle":"2022-06-20T13:14:24.676358Z","shell.execute_reply.started":"2022-06-20T13:14:24.425564Z","shell.execute_reply":"2022-06-20T13:14:24.675573Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaForSequenceClassification, RobertaConfig, AdamW\n\nconfig = RobertaConfig.from_pretrained(\n    \"/kaggle/working/PhoBERT_base_transformers/config.json\", from_tf=False, num_labels = len(labels), output_hidden_states=False,\n)\nBERT_SA = RobertaForSequenceClassification.from_pretrained(\n    \"/kaggle/working/PhoBERT_base_transformers/model.bin\",\n    config=config\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:14:24.677743Z","iopub.execute_input":"2022-06-20T13:14:24.678100Z","iopub.status.idle":"2022-06-20T13:14:26.321370Z","shell.execute_reply.started":"2022-06-20T13:14:24.678063Z","shell.execute_reply":"2022-06-20T13:14:26.320487Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"BERT_SA.cuda()\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:14:26.322593Z","iopub.execute_input":"2022-06-20T13:14:26.323019Z","iopub.status.idle":"2022-06-20T13:14:26.467867Z","shell.execute_reply.started":"2022-06-20T13:14:26.322980Z","shell.execute_reply":"2022-06-20T13:14:26.467055Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import f1_score, accuracy_score\n\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    \n    return accuracy_score(pred_flat, labels_flat)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:14:26.469167Z","iopub.execute_input":"2022-06-20T13:14:26.469782Z","iopub.status.idle":"2022-06-20T13:14:26.475864Z","shell.execute_reply.started":"2022-06-20T13:14:26.469739Z","shell.execute_reply":"2022-06-20T13:14:26.475150Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import random\nimport time\nfrom tqdm import notebook\ndevice = 'cuda'\nepochs = 100\nmax = 0\n\nparam_optimizer = list(BERT_SA.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\n\noptimizer = AdamW(optimizer_grouped_parameters, lr=5e-7, correct_bias=False,no_deprecation_warning=True)\n\n\nfor epoch_i in range(0, epochs):\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    total_loss = 0\n    BERT_SA.train()\n    train_accuracy = 0\n    nb_train_steps = 0\n    start_time=time.time()\n    for step, batch in notebook.tqdm(enumerate(train_dataloader)):\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        BERT_SA.zero_grad()\n        outputs = BERT_SA(b_input_ids, \n            token_type_ids=None, \n            attention_mask=b_input_mask, \n            labels=b_labels)\n        loss = outputs[0]\n        total_loss += loss.item()\n        \n        logits = outputs[1].detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        tmp_train_accuracy = flat_accuracy(logits, label_ids)\n        train_accuracy += tmp_train_accuracy\n        nb_train_steps += 1\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(BERT_SA.parameters(), 1.0)\n        optimizer.step()\n        \n    avg_train_loss = total_loss / len(train_dataloader)\n    print(\" Accuracy: {0:.4f}\".format(train_accuracy/nb_train_steps))\n    print(\" Average training loss: {0:.4f}\".format(avg_train_loss))\n    BERT_SA.save_pretrained('/content/drive/MyDrive/Deep/modelbert/{}'.format(epoch_i+1))\n    print(\"Done training this epoch for {} seconds\".format(time.time()-start_time))\n    print(\"Running Validation...\")\n    BERT_SA.eval()\n    start_time=time.time()\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    for batch in notebook.tqdm(test_dataloader):\n\n        batch = tuple(t.to(device) for t in batch)\n\n        b_input_ids, b_input_mask, b_labels = batch\n\n        with torch.no_grad():\n            outputs = BERT_SA(b_input_ids, \n            token_type_ids=None, \n            attention_mask=b_input_mask)\n            logits = outputs[0]\n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n\n            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n\n            eval_accuracy += tmp_eval_accuracy\n            nb_eval_steps += 1\n    if max < eval_accuracy/nb_eval_steps:\n      max = eval_accuracy/nb_eval_steps\n      e = epoch_i\n    print('logits:    {}'.format(np.argmax(logits, axis=1).flatten()))\n    print('labels_id: {}'.format(label_ids))\n    print(\" Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))\n    print(\"Evaluating in {} seconds\".format(time.time()-start_time))\nprint(\"Training complete!\")\nprint('Best Valid acc : {}, epoch: {}'.format(max, e+1))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T13:17:52.639350Z","iopub.execute_input":"2022-06-20T13:17:52.639714Z","iopub.status.idle":"2022-06-20T17:20:52.734328Z","shell.execute_reply.started":"2022-06-20T13:17:52.639683Z","shell.execute_reply":"2022-06-20T17:20:52.733168Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"torch.save(BERT_SA,\"/kaggle//bert.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:06:56.518122Z","iopub.execute_input":"2022-06-20T18:06:56.518506Z","iopub.status.idle":"2022-06-20T18:06:57.491202Z","shell.execute_reply.started":"2022-06-20T18:06:56.518476Z","shell.execute_reply":"2022-06-20T18:06:57.490373Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def predict(model, bpe, sense, vocab):\n  sense=standard_data([sense])[0]\n  sense=rdrsegmenter.tokenize(sense)\n  sense=' '.join([' '.join(x) for x in sense])\n  subwords = '<s> ' + bpe.encode(sense) + ' </s>'\n  encoded_sent = vocab.encode_line(subwords, append_eos=True, add_if_not_exist=False).long().tolist()\n  encoded_sent = pad_sequences([encoded_sent], maxlen=250, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n  mask = [int(token_id > 0) for token_id in encoded_sent[0]]\n\n\n  encoded_sent = torch.tensor(encoded_sent).cuda()\n  mask = torch.tensor(mask).cuda()\n  encoded_sent = torch.reshape(encoded_sent, (1, 250))\n  mask = torch.reshape(mask, (1, 250))\n\n  with torch.no_grad():\n    outputs = model(encoded_sent, \n      token_type_ids=None, \n      attention_mask=mask)\n    logits = outputs[0]\n  return int(torch.argmax(logits))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:19:15.924399Z","iopub.execute_input":"2022-06-20T18:19:15.924966Z","iopub.status.idle":"2022-06-20T18:19:15.935432Z","shell.execute_reply.started":"2022-06-20T18:19:15.924929Z","shell.execute_reply":"2022-06-20T18:19:15.934578Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"id2label=dict([id,label] for label,id in zip(label2id.keys(),label2id.values()))\nid2label[predict(BERT_SA,bpe,\"Nicolo Zaniolo, sút, vào, anh không ăn mừng. Vâng anh không ăn mừng\",vocab)]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T18:20:39.692007Z","iopub.execute_input":"2022-06-20T18:20:39.692929Z","iopub.status.idle":"2022-06-20T18:20:39.731755Z","shell.execute_reply.started":"2022-06-20T18:20:39.692890Z","shell.execute_reply":"2022-06-20T18:20:39.730469Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}