{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec+LSTM",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9ufmI_VWZWe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from collections import defaultdict \n",
        "from tensorflow import keras\n",
        "from keras import layers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "P6ekYI_XWbd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import sys\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Embedding"
      ],
      "metadata": {
        "id": "2ZlvPihGWf8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/Data/viettel_train_input_no_tokenize.txt\",\"r\",encoding='UTF-8') as f:\n",
        "  X_train=f.read().splitlines()\n",
        "with open(\"/Data/viettel_test_input_no_tokenize.txt\",\"r\",encoding='UTF-8') as f:\n",
        "  X_test=f.read().splitlines()\n",
        "with open(\"/Data/viettel_train_label.txt\",\"r\",encoding='UTF-8') as f:\n",
        "  y_train=f.read().splitlines()\n",
        "with open(\"/Data/viettel_test_label.txt\",\"r\",encoding='UTF-8') as f:\n",
        "  y_test=f.read().splitlines()"
      ],
      "metadata": {
        "id": "V2ovMxoLWnRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = gensim.models.Word2Vec(sentences=X_train, vector_size=500, window=5, min_count=2, workers=4,iter=50)\n",
        "w2v_weights = w2v_model.wv.vectors\n",
        "vocab_size, embedding_size = w2v_weights.shape\n",
        "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))"
      ],
      "metadata": {
        "id": "UvkwH4A_WnzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2token(word):\n",
        "    try:\n",
        "        return w2v_model.wv.vocab[word].index\n",
        "    except KeyError:\n",
        "        return 0\n",
        "def token2word(token):\n",
        "    return w2v_model.wv.index2word[token]"
      ],
      "metadata": {
        "id": "NjEBG8taXDGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "n_samples = 500\n",
        "# Sample random words from model dictionary\n",
        "random_i = random.sample(range(vocab_size), n_samples)\n",
        "random_w = [token2word(i) for i in random_i]\n",
        "\n",
        "# Generate Word2Vec embeddings of each word\n",
        "word_vecs = np.array([w2v_model[w] for w in random_w])\n",
        "\n",
        "# Apply t-SNE to Word2Vec embeddings, reducing to 2 dims\n",
        "tsne = TSNE()\n",
        "tsne_e = tsne.fit_transform(word_vecs)\n",
        "\n",
        "# Plot t-SNE result\n",
        "plt.figure(figsize=(32, 32))\n",
        "plt.scatter(tsne_e[:, 0], tsne_e[:, 1], marker='o', c=range(len(random_w)), cmap=plt.get_cmap('Spectral'))\n",
        "\n",
        "for label, x, y, in zip(random_w, tsne_e[:, 0], tsne_e[:, 1]):\n",
        "    plt.annotate(label,\n",
        "                 xy=(x, y), xytext=(0, 15),\n",
        "                 textcoords='offset points', ha='right', va='bottom',\n",
        "                 bbox=dict(boxstyle='round, pad=0.2', fc='yellow', alpha=0.1))"
      ],
      "metadata": {
        "id": "YIkIijsTXHW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=[[word2token(word) for word in text] for text in X_train]\n",
        "X_train= pad_sequences(X_train)\n",
        "X_test=[[word2token(word) for word in text] for text in X_test]\n",
        "X_test=pad_sequences(X_test,maxlen=X_train.shape[1])"
      ],
      "metadata": {
        "id": "tiOXSk_XXLol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=list(set(y_train))\n",
        "label2id=dict([label,id] for id,label in enumerate(labels))\n",
        "y_train_vectorized=np.array([label2id[label] for label in y_train])\n",
        "y_test_vectorized=np.array([label2id[label] for label in y_test])"
      ],
      "metadata": {
        "id": "wsDiMXhqXO-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_weights = w2v_model.wv.vectors"
      ],
      "metadata": {
        "id": "6ZOsHEMUXhtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size,output_dim=embedding_size,weights=[w2v_weights],trainable=False,input_length=X_train.shape[1]))\n",
        "model.add(LSTM(256,return_sequences=True))\n",
        "model.add(LSTM(64,return_sequences=True))\n",
        "model.add(LSTM(128,return_sequences=False))\n",
        "model.add(Dense(len(labels),activation=\"softmax\"))\n",
        "model.summary()\n",
        "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['acc'])\n",
        "checkpoint_filepath=\"/Word2Vec+LSTM-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "batch = 64\n",
        "epochs = 40\n",
        "checkpoint=ModelCheckpoint(checkpoint_filepath, \n",
        "                monitor = 'val_acc', \n",
        "                verbose = 1, \n",
        "                save_best_only = True, \n",
        "                mode = 'max')\n",
        "callbacks_list = [checkpoint]\n",
        "batch = 64\n",
        "epochs = 80\n",
        "history=model.fit(X_train,y_train_vectorized,batch,epochs,validation_data=(X_test,y_test_vectorized),callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "NNqAqQboXkNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/Word2Vec+LSTM-weights-improvement-61-0.66.hdf5\")\n",
        "start_time=time.time()\n",
        "logits=model.predict(X_test)\n",
        "print(\"Inference in {} seconds\".format((time.time()-start_time)/len(X_test)))"
      ],
      "metadata": {
        "id": "kg8bjsXhXo-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "cnf_matrix = confusion_matrix(y_test_vectorized, pred)\n",
        "df_cm = pd.DataFrame(cnf_matrix)\n",
        "plt.figure(figsize=(30,10))\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DCXaJ3LlXs38"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}