{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whKivEWVySFk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from collections import defaultdict \n",
        "from tensorflow import keras\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "UfU9EebryTbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from os import listdir\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Embedding"
      ],
      "metadata": {
        "id": "qjiax4VeyVFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/Data/viettel_train_input_no_tokenize.txt\",\"r\",encoding='UTF-8') as f:\n",
        "  X_train=f.read().splitlines()\n",
        "with open(\"/Data/viettel_test_input_no_tokenize.txt\",\"r\",encoding='UTF-8') as f:\n",
        "  X_test=f.read().splitlines()\n",
        "with open(\"/Data/viettel_train_label.txt\",\"r\",encoding='UTF-8') as f:\n",
        "  y_train=f.read().splitlines()\n",
        "with open(\"/Data/viettel_test_label.txt\",\"r\",encoding='UTF-8') as f:\n",
        "  y_test=f.read().splitlines()"
      ],
      "metadata": {
        "id": "DwFLnlgXyWFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def txtTokenizer(texts):\n",
        "    tokenizer = Tokenizer()\n",
        "    # fit the tokenizer on our text\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    # get all words that the tokenizer knows\n",
        "    word_index = tokenizer.word_index\n",
        "    return tokenizer, word_index"
      ],
      "metadata": {
        "id": "z2AyIBNiyd1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, word_index = txtTokenizer(X_train)"
      ],
      "metadata": {
        "id": "VN7UzHXSyfdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
        "X_train_encoded= pad_sequences(X_train_encoded)\n",
        "X_test_encoded = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_encoded= pad_sequences(X_test_encoded,maxlen=X_train_encoded.shape[1])"
      ],
      "metadata": {
        "id": "xAsodsy2yg5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=list(set(y_train))\n",
        "label2id=dict([label,id] for id,label in enumerate(labels))\n",
        "y_train_vectorized=np.array([label2id[label] for label in y_train])\n",
        "y_test_vectorized=np.array([label2id[label] for label in y_test])"
      ],
      "metadata": {
        "id": "1WOvkWGDyk06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "vocab_size=len(word_index)\n",
        "embedding_size=300\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(vocab_size,embedding_size,input_length=X_train_encoded.shape[1]))\n",
        "model2.add(LSTM(128,return_sequences=True))\n",
        "model2.add(LSTM(256,return_sequences=False))\n",
        "model2.add(Dense(len(labels),activation=\"softmax\"))\n",
        "model2.summary()\n",
        "model2.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['acc'])\n",
        "checkpoint_filepath=\"/LSTM-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "batch = 64\n",
        "epochs = 40\n",
        "checkpoint=ModelCheckpoint(checkpoint_filepath, \n",
        "                monitor = 'val_acc', \n",
        "                verbose = 1, \n",
        "                save_best_only = True, \n",
        "                mode = 'max')\n",
        "callbacks_list = [checkpoint]\n",
        "history2=model2.fit(X_train_encoded,y_train_vectorized,batch,epochs,validation_data=(X_test_encoded,y_test_vectorized),callbacks = callbacks_list)"
      ],
      "metadata": {
        "id": "I_1whtKTylIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.load_weights(\"/LSTM-weights-improvement-35-0.84.hdf5\")\n",
        "start_time=time.time()\n",
        "pred = model2.predict(X_test_encoded) \n",
        "print(\"Inference in {} seconds\".format((time.time()-start_time)/len(X_test_encoded)))\n"
      ],
      "metadata": {
        "id": "tq0Xs1K7yy0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history2.history['acc'])\n",
        "plt.plot(history2.history['val_acc'])\n",
        "plt.title('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['acc', 'val_acc'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sT88ECvOyn-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cnf_matrix = confusion_matrix(y_test_vectorized, pred)\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "df_cm = pd.DataFrame(cnf_matrix)\n",
        "plt.figure(figsize=(30,10))\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z5ae4ZvNyqdb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}